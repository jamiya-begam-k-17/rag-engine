<div align="center">

# RAG Engine

### *A Collaborative Project under ReadyTensor AAIDC Programme*

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)
[![React](https://img.shields.io/badge/React-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://reactjs.org/)
[![LangChain](https://img.shields.io/badge/ğŸ¦œ_LangChain-121212?style=for-the-badge)](https://www.langchain.com/)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-FF6584?style=for-the-badge)](https://www.trychroma.com/)
[![SQLite](https://img.shields.io/badge/SQLite-003B57?style=for-the-badge&logo=sqlite&logoColor=white)](https://www.sqlite.org/)

**A near production-grade Retrieval-Augmented Generation system that transforms static documents into an interactive, AI-powered knowledge base. By combining semantic search with state-of-the-art Large Language Models, RAG Engine delivers accurate, context-aware responses grounded in your specific documents.**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [API Docs](#-react-version-backend-documentation) â€¢ [Team](#-team)

</div>

---

## ğŸš€ Features

- **Multi-LLM Support**: Compatible with OpenAI GPT, Google Gemini, and Groq (Llama) models
- **Document Processing**: Handles PDF and TXT files with intelligent chunking
- **Vector Search**: Uses ChromaDB with sentence transformers for semantic search
- **Smart Deduplication**: Automatically detects and reuses previously processed documents
- **Session Management**: SQLite-based session and chat history tracking
- **RESTful API**: FastAPI backend with comprehensive endpoints
- **Stateless & Stateful**: Works both as a CLI tool and API service

## ğŸ“‹ Table of Contents

- [Team](#-team)
- [Architecture](#-architecture)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
  - [Streamlit Version](#streamlit-version)
  - [React Version](#react-version)
- [React Version Backend Documentation](#-react-version-backend-documentation)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [Limitations](#-limitations)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

## ğŸ‘¥ Team

This project was developed as a collaborative effort for ReadyTensor's AAIDC Course (Project 1).

### Team Members

**1. Adarsh Raj - Lead Developer & AI/ML Architect**\
**GitHub: [@AdarshRaj2028](https://github.com/AdarshRaj2028)**
- Project lead and coordinator
- Overall architecture and pipeline design
- Created Streamlit version and connected with the backend
- RAG logic implementation (retriever, embedder, vector DB flow)
- SQLite database setup and schema design
- Smart caching system (SHA-256 + UUID5)
- Helped in configuring vector database
- Final testing and debugging

**2. Jamiya Begam - Frontend Developer & API Integration**\
**GitHub: [@jamiya-begam-k-17](https://github.com/jamiya-begam-k-17)**
- React UI development and component design
- Configured FastAPI
- Frontend-backend API integration using FastAPI
- Chat interface and user interaction design
- API request/response handling
- Consistent styling and usability
- Helped in testing both UI versions

**3. Natalie Wanjiru - Programming & Documentation Support**\
**GitHub: [@Wanjiru-Natalie](https://github.com/Wanjiru-Natalie)**
- RAG code module development
- Document loading, chunking, embedding implementation
- Vector database handling
- Technical documentation for backend
- System architecture documentation
- Overall architecture reviewer

**4. Sunday Victor - AI/ML Theory & Optimization Specialist**\
**GitHub: [@Sunvic567](https://github.com/Sunvic567)**
- Embedding model research and evaluation
- RAG performance testing
- Retrieval parameter fine-tuning (chunk size, overlap, n_results)
- Model performance benchmarking
- Comparison reports and recommendations
- Theoretical documentation and RAG overview
- Laid the foundation of vector database

### Collaboration

This project showcases effective team collaboration with clear role division while maintaining flexibility for cross-functional contributions. Each team member brought unique expertise to create a robust, near production-ready RAG system.

- **Project Timeline**: September 2025 - December 2025
- **Course**: ReadyTensor AAIDC Programme
- **Project Type**: Retrieval-Augmented Generation System

## ğŸ—‚ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERACTION                     â”‚
â”‚                  (Streamlit/React)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Upload Document    â”‚
              â”‚ (PDF or TXT)        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚          SMART CACHING CHECK                 â”‚
         â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
         â”‚  1. Compute SHA-256 hash from file byte      â”‚
         â”‚  2. Generate deterministic UUID5 from hash   â”‚
         â”‚  3. Query SQLite: document_id exists?        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚      Cache Hit?      â”‚
               â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
                   â”‚              â”‚
              YES  â”‚              â”‚  NO
                   â”‚              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  INSTANT     â”‚    â”‚  FULL PROCESSING     â”‚
        â”‚  REUSE       â”‚    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚    â”‚  1. Extract text     â”‚
        â”‚  â€¢ Load      â”‚    â”‚  2. Chunk text       â”‚
        â”‚    existing  â”‚    â”‚     (1500 chars,     â”‚
        â”‚    chunks    â”‚    â”‚      150 overlap)    â”‚
        â”‚  â€¢ Link to   â”‚    â”‚  3. Generate         â”‚
        â”‚    session   â”‚    â”‚     embeddings       â”‚
        â”‚  â€¢ <1 sec    â”‚    â”‚     (SentenceT.)     â”‚
        â”‚              â”‚    â”‚  4. Store in         â”‚
        â”‚              â”‚    â”‚     ChromaDB         â”‚
        â”‚              â”‚    â”‚  5. Update SQLite    â”‚
        â”‚              â”‚    â”‚  â€¢ ~7-10 sec         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚             â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  READY TO QUERY  â”‚
                â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                â”‚   â€¢ Document in  â”‚
                â”‚     vector DB    â”‚
                â”‚   â€¢ Metadata in  â”‚
                â”‚     SQLite       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   User Question       â”‚
                â”‚   (Natural Language)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   RETRIEVAL PIPELINE       â”‚
                â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
                â”‚   1. Generate query        â”‚
                â”‚      embedding             â”‚
                â”‚   2. Vector similarity     â”‚
                â”‚      search (cosine)       â”‚
                â”‚   3. Retrieve top-k        â”‚
                â”‚      chunks (default: 3)   â”‚
                â”‚   â€¢ ~0.5-1 sec             â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   GENERATION PIPELINE      â”‚
                â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
                â”‚   1. Assemble context      â”‚
                â”‚      from chunks           â”‚
                â”‚   2. Format RAG prompt     â”‚
                â”‚      template              â”‚
                â”‚   3. Call LLM API          â”‚
                â”‚      (OpenAI/Groq/Gemini)  â”‚
                â”‚   4. Generate response     â”‚
                â”‚   â€¢ ~2-4 sec               â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Response Delivery     â”‚
                â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                â”‚  â€¢ AI-generated answer â”‚
                â”‚  â€¢ Source citations    â”‚
                â”‚  â€¢ Metadata tracking   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Core Components:**

1. **FastAPI Server** (`main.py`): HTTP API endpoints
2. **RAGAssistant** (`app.py`): Main orchestration logic
3. **VectorDB** (`vectordb.py`): ChromaDB wrapper for embeddings
4. **RAGDatabase** (`database.py`): SQLite for sessions, documents, and messages
5. **Utils** (`utils.py`): File validation and processing

## âœ… Prerequisites

- Python 3.8+
- pip or conda
- API key for at least one LLM provider:
  - OpenAI API key
  - Google AI (Gemini) API key
  - Groq API key

## ğŸ“¦ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/AdarshRaj2028/rag-engine.git
cd rag-engine
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# On Linux/Mac
source venv/bin/activate

# On Windows
venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

## âš™ï¸ Configuration

### Environment Variables

There is a `.env.example` template.

```bash
cp .env.example .env
```
This will create a configured `.env` file for you to run the Streamlit version. 

### File Limits

The system has built-in limits to prevent abuse:

- **PDF**: Maximum 100 pages
- **TXT**: Maximum 10 MB
- **Upload Size**: Maximum 25 MB per file

You can modify these in `utils.py`:

```python
MAX_PAGES = 100
MAX_TXT_SIZE_MB = 10
```

## ğŸ¯ Usage

The RAG Engine offers two different interfaces to suit your needs:

### Streamlit Version

Beautiful web interface with glassmorphism design - perfect for non-technical users:

```bash
# Start the Streamlit app
cd src
streamlit run frontend_app.py
```

**Features:**

- ğŸ¨ Modern glassmorphism UI with gradient backgrounds
- ğŸ“ Drag-and-drop file upload
- ğŸ’¬ Real-time chat interface with typing animation
- ğŸ“š Source attribution with expandable context
- ğŸ”„ Smart caching indicators
- ğŸ“Š Document metadata display (chunks, pages, file size)

### React Version

For production deployments and integration with frontends:

```bash
# Start the FastAPI based backend server first
cd src
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

```bash
# Then start the React based frontend
cd rag-ui
npm start
```

**Extra Features:**

- ğŸ”‘ BYOK (Bring Your Own Key) concept implemented
- ğŸ“ Separated key fetching from database
- ğŸ” Separated folder to keep data segregated from Streamlit Version

The API will be available at `http://localhost:8000`

**Interactive API Docs**: Visit `http://localhost:8000/docs` for Swagger UI

## ğŸ“¡ React Version Backend Documentation

### Base URL

```
http://localhost:8000
```

### Endpoints

#### 1. Health Check

```http
GET /health
```

**Response:**
```json
{
  "status": "ok"
}
```

---

#### 2. Save API Key

```http
POST /api-key
Content-Type: application/json
```

**Request Body:**
```json
{
  "api_key": "your-api-key",
  "model": "gemini-2.0-flash-exp"
}
```

**Response:**
```json
{
  "status": "success",
  "message": "API key saved for gemini-2.0-flash-exp"
}
```

---

#### 3. Check API Key Status

```http
GET /api-key-status
```

**Response:**
```json
{
  "has_api_key": true,
  "model": "gemini-2.0-flash-exp",
  "success": true
}
```

---

#### 4. Upload Document

```http
POST /upload
Content-Type: multipart/form-data
```

**Form Data:**
- `file`: PDF or TXT file

**Response:**
```json
{
  "status": "success",
  "message": "Document processed successfully",
  "session_id": "abc123...",
  "filename": "document.pdf",
  "file_size": 1048576,
  "file_type": "application/pdf",
  "page_count": 15,
  "chunk_count": 42,
  "processing_time": 3.45,
  "from_cache": false,
  "uploaded_at": "2024-01-15T10:30:00Z"
}
```

---

#### 5. Query Document

```http
POST /query
Content-Type: application/json
```

**Request Body:**
```json
{
  "session_id": "abc123...",
  "question": "What are the main findings?",
  "n_results": 3
}
```

**Response:**
```json
{
  "answer": "The main findings indicate...",
  "sources": [
    "Chunk 1 text...",
    "Chunk 2 text...",
    "Chunk 3 text..."
  ],
  "status": "success",
  "session_id": "abc123..."
}
```

---

#### 6. Send Message

```http
POST /messages
Content-Type: application/json
```

**Request Body:**
```json
{
  "session_id": "abc123...",
  "content": "Explain the methodology"
}
```

**Response:**
```json
{
  "message_id": null,
  "content": "The methodology involves..."
}
```

---

#### 7. Get Chat History

```http
GET /messages/{session_id}
```

**Response:**
```json
[
  {
    "role": "assistant",
    "content": "Hello! I've loaded your document...",
    "timestamp": "2024-01-15T10:30:00"
  },
  {
    "role": "user",
    "content": "What is this about?",
    "timestamp": "2024-01-15T10:31:00"
  },
  {
    "role": "assistant",
    "content": "This document discusses...",
    "timestamp": "2024-01-15T10:31:05"
  }
]
```

---

#### 8. Get Document Info

```http
GET /document/{session_id}
```

**Response:**
```json
{
  "document_id": "def456...",
  "file_name": "document.pdf",
  "file_hash": "sha256...",
  "chunk_count": 42,
  "collection_name": "doc_def456...",
  "status": "completed",
  "file_size": 1048576,
  "file_type": "application/pdf",
  "page_count": 15,
  "uploaded_at": "2024-01-15T10:30:00"
}
```

## ğŸ“ Project Structure

```
rag-engine/
â”œâ”€â”€ src/                          # Backend source code
â”‚   â”œâ”€â”€ main.py                   # FastAPI app, routes, CORS, file upload
â”‚   â”œâ”€â”€ app.py                    # RAG orchestration, LLM init, query pipeline
â”‚   â”œâ”€â”€ vectordb.py               # ChromaDB wrapper, chunking, embeddings
â”‚   â”œâ”€â”€ database.py               # SQLite operations, smart caching logic
â”‚   â”œâ”€â”€ utils.py                  # File validation, PDF parsing
â”‚   â””â”€â”€ frontend_app.py           # Streamlit UI with glassmorphism design
â”‚
â”œâ”€â”€ rag-ui/                       # React frontend 
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/           # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat.jsx          # Chat interface with message history
â”‚   â”‚   â”‚   â”œâ”€â”€ Upload.jsx        # File upload with progress
â”‚   â”‚   â”‚   â”œâ”€â”€ Settings.jsx      # API key configuration
â”‚   â”‚   â”‚   â””â”€â”€ Sidebar.jsx       # Navigation sidebar
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js            # Axios client for backend API
â”‚   â”‚   â”œâ”€â”€ App.jsx               # Root component with routing
â”‚   â”‚   â”œâ”€â”€ App.css               # Global styles
â”‚   â”‚   â””â”€â”€ index.js              # Entry point
â”‚   â”œâ”€â”€ public/                   # Static assets (index.html, favicon)
â”‚   â”œâ”€â”€ package.json              # Node dependencies
â”‚   â””â”€â”€ tailwind.config.js        # Tailwind CSS configuration
â”‚
â”œâ”€â”€ data/                         # Document storage (gitignored)
â”‚   â””â”€â”€ (uploaded PDFs/TXTs)      # Temporary storage for processing
â”‚
â”œâ”€â”€ chroma_db/                    # Vector database (gitignored)
â”‚   â””â”€â”€ (persistent embeddings)   # ChromaDB collection files
â”‚
â”œâ”€â”€ rag_engine.db                 # SQLite database (gitignored)
â”‚                                 # Contains: sessions, documents, messages
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env.example                  # Environment variable template
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ README.md                     # This file
```

## ğŸ”§ How It Works

### 1. Document Upload Flow

```
1. User uploads PDF/TXT â†’ File validation (size, type, content)
2. Compute SHA256 hash â†’ Check if document already exists
3. If new:
   - Generate unique document_id
   - Create ChromaDB collection
   - Split into chunks (1500 chars, 150 overlap)
   - Generate embeddings using sentence-transformers
   - Store chunks in vector database
4. If existing:
   - Reuse existing chunks (no reprocessing, saves time)
5. Create session â†’ Link session to document â†’ Return session_id
```

### 2. Query Flow

```
1. User sends question â†’ Validate session_id
2. Generate query embedding
3. Search ChromaDB â†’ Retrieve top K similar chunks (default: 3)
4. Combine chunks into context
5. Build prompt: "Use the following context to answer: {context}\nQuestion: {question}"
6. Send to LLM â†’ Get response
7. Save to chat history â†’ Return answer + sources
```

### 3. Deduplication Strategy

The system uses content-based deduplication:

- **Document ID**: `UUID5(SHA256(file_content))`
- Same content = same ID = reuse existing chunks
- Different sessions can share the same document chunks
- Saves processing time and storage space

## âš ï¸ Limitations

- **PDF Limitations**: 
  - Scanned PDFs without OCR are not supported
  - Password-protected PDFs cannot be processed
  - Maximum 100 pages per document
  
- **Text Extraction**:
  - Tables and complex layouts may not parse correctly
  - Images and diagrams are ignored
  
- **Embedding Model**:
  - Fixed embedding model (all-MiniLM-L6-v2)
  - Cannot change after documents are processed
  
- **Context Window**:
  - Only retrieves top 3 chunks by default
  - May miss relevant information if query is too broad

## ğŸ›  Troubleshooting

### Issue: "No API key found"

**Solution**: Ensure your `.env` file exists and contains a valid API key:

```bash
# Check if .env exists
ls -la .env

# Verify contents
cat .env
```

### Issue: "PDF contains no extractable text"

**Solution**: The PDF is likely scanned. Use a text-based PDF or run OCR first.

### Issue: "ModuleNotFoundError"

**Solution**: Reinstall dependencies:

```bash
pip install -r requirements.txt --force-reinstall
```

### Issue: ChromaDB collection errors

**Solution**: Delete and recreate the database:

```bash
rm -rf chroma_db/
rm rag_engine.db
python app.py  # Will recreate on startup
```

### Issue: API returns 400 "API key required"

**Solution**: Upload an API key first:

```bash
curl -X POST http://localhost:8000/api-key \
  -H "Content-Type: application/json" \
  -d '{"api_key": "your-key", "model": "gemini-2.0-flash-exp"}'
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit your changes: `git commit -m 'Add feature'`
4. Push to the branch: `git push origin feature-name`
5. Open a Pull Request


## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain**: For document processing and LLM integrations
- **ChromaDB**: For vector database functionality
- **Sentence Transformers**: For embedding generation
- **FastAPI**: For the REST API framework